{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spam dataset Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from csv import reader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from random import randrange\n",
    "import operator\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "# prediction values\n",
    "def pred_val(theta, X, hard=True):\n",
    "    pred_prob = logistic_val_func(theta, X)\n",
    "    pred_value = np.where(pred_prob > 0.5, 1, 0)\n",
    "    if hard:\n",
    "        return pred_value\n",
    "    else:\n",
    "        return pred_prob\n",
    "\n",
    "\n",
    "def logistic_grad_func(theta, x, y):\n",
    "    # compute gradient\n",
    "    m = x.shape[0]\n",
    "    y_hat = logistic_val_func(theta, x)\n",
    "    x = np.c_[np.ones(x.shape[0]), x]\n",
    "    grad = (1.0 / m) * np.sum((y_hat - y) * x, axis=0)\n",
    "    return grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    # sigmoid function\n",
    "    sig = 1.0 / (1.0 + np.exp(-1.0 * x))\n",
    "    return sig\n",
    "\n",
    "\n",
    "def logistic_val_func(theta, x):\n",
    "    return sigmoid(np.dot(np.c_[np.ones(x.shape[0]), x], theta.T))\n",
    "\n",
    "\n",
    "def logistic_cost_func(theta, x, y):\n",
    "    # compute cost (loss)\n",
    "    y_hat = logistic_val_func(theta, x)\n",
    "    cost = np.sum(-1.0 * y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat), axis=0)\n",
    "    cost *= 1.0 / x.shape[0]\n",
    "    return cost\n",
    "\n",
    "# def logistic_cost_func(theta, X, y):\n",
    "#     # compute cost (loss)\n",
    "#     y_hat = logistic_val_func(theta, X)\n",
    "    \n",
    "#     # since sigmoid(4e+01) is already 1.0 in python\n",
    "#     # np.log(1-y_hat) cannot be calculated properly\n",
    "#     # -log(1-h) = log(1+exp(z))\n",
    "#     # where h = 1/(1+exp(-z))\n",
    "#     # --> avoid RuntimeWarning: divide by zero encountered in log\n",
    "#     #       and RuntimeWarning: invalid value encountered in double_scalars\n",
    "    \n",
    "#     z = np.dot(np.c_[np.ones(X.shape[0]), X], theta.T)\n",
    "# #     cost = -np.sum(y * np.log(1 + np.exp(-z))) + np.sum((1 - y) * np.log(1+np.exp(z)))\n",
    "#     cost = np.sum(y * np.log(y_hat)) - np.sum((1 - y) * np.log(1+np.exp(z))) # very slow\n",
    "# #     cost = np.sum(y * np.log(y_hat)) + np.sum((1 - y) * np.log(1-y_hat)) # very fast\n",
    "#     cost *= 1.0 / X.shape[0]\n",
    "# #     print(\"logistic_cost_func completed\")\n",
    "#     return -cost\n",
    "\n",
    "\n",
    "def logistic_grad_desc(theta, X_train, Y_train, lr, max_iter, tolerance):\n",
    "    cost_iter = []\n",
    "    cost = logistic_cost_func(theta, X_train, Y_train)\n",
    "    cost_iter.append(cost)\n",
    "    cost_change = 1\n",
    "    i = 1\n",
    "    while cost_change > tolerance and i < max_iter:\n",
    "        pre_cost = cost\n",
    "        # compute gradient\n",
    "        grad = logistic_grad_func(theta, X_train, Y_train)\n",
    "        theta -= lr * grad\n",
    "        cost = logistic_cost_func(theta, X_train, Y_train)\n",
    "        cost_iter.append(cost)\n",
    "        cost_change = abs(pre_cost - cost)\n",
    "        i += 1\n",
    "    return theta, cost_iter\n",
    "\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'r') as dest_f:\n",
    "        data_iter = reader(dest_f, delimiter=',', quotechar='\"')\n",
    "        data = [data for data in data_iter]\n",
    "        data_array = np.asarray(data)\n",
    "    return data_array\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "def logistic_regression(dataset, n_folds, lr, max_iter, tolerance):\n",
    "    # split dataset into n-folds\n",
    "    dataset_split = cross_validation_split(dataset, n_folds)\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    sklearn_acc_train = []\n",
    "    sklearn_acc_test = []\n",
    "\n",
    "    precision_test = []\n",
    "    recall_test = []\n",
    "             \n",
    "    for i in range(n_folds):\n",
    "        test = np.array(dataset_split[i])\n",
    "        train = list(dataset_split)\n",
    "        train.pop(i)\n",
    "        # combine the remaining lists of folder into one\n",
    "        train = np.array(reduce(operator.add, train))\n",
    "        \n",
    "        # Normalize X_Train\n",
    "        X_train = train[:, :-1]\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        \n",
    "        #Get the mean and std to normalize the test dataset\n",
    "        X_test = test[:, :-1]\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        Y_train = train[:, -1]\n",
    "        Y_test = test[:,-1]\n",
    "        \n",
    "        Y_train = Y_train[:, None]\n",
    "        Y_test = Y_test[:, None]\n",
    "\n",
    "        # Logitic regression\n",
    "        #  Initialize the weights for the gradient descent algorithm to all zeros\n",
    "        theta = np.zeros((1, X_train.shape[1] + 1))\n",
    "        \n",
    "        # Initialize the weights for the gradient descent algorithm between 0 and 1\n",
    "        #theta = np.random.rand(1, X_train.shape[1] + 1)\n",
    "        \n",
    "        # Initialize the weights for the gradient descent algorithm between -1 and 1\n",
    "        #theta = np.random.uniform(-1, 1, X_train.shape[1] + 1)\n",
    "        \n",
    "        \n",
    "        fitted_theta, cost_iter = logistic_grad_desc(theta, X_train, Y_train, lr, max_iter, tolerance)\n",
    "        \n",
    "       # _, cost_iter_2 = logistic_grad_desc(theta, X_train, Y_train, lr, max_iter, tolerance=0.001)\n",
    "        \n",
    "        #print ('cost_iter_2', cost_iter_2)\n",
    "        # choose fold one \n",
    "        if i == 0:\n",
    "            plt.figure()\n",
    "            plt.plot(cost_iter, label = \"cost_iter\")\n",
    "           # plt.plot(cost_iter_2, label = \"cost_iter_2\")\n",
    "            plt.grid()\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Logistic_cost')\n",
    "            plt.legend(bbox_to_anchor=(1.05,1), loc=2, shadow=True)\n",
    "            plt.show()\n",
    "       \n",
    "        predict_test = pred_val(fitted_theta, X_test)\n",
    "        predict_train = pred_val(fitted_theta, X_train)\n",
    "        acc_test.append(np.sum(predict_test == Y_test) * 1.0 / X_test.shape[0])\n",
    "        acc_train.append(np.sum(predict_train == Y_train) * 1.0 / X_train.shape[0])\n",
    "       \n",
    "        #Precision: TP / TP + FP \n",
    "        #Recall: TP / TP + FN\n",
    "        TP = np.sum(Y_test == 1)\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for j in range(len(Y_test)):\n",
    "            if predict_test[j] == 1 and Y_test[j] == 0:\n",
    "                FP += 1\n",
    "            if predict_test[j] == 0 and Y_test[j] == 1:\n",
    "                FN += 1\n",
    "        \n",
    "        \n",
    "        precision = TP * 1. / (TP + FP)\n",
    "        recall = TP * 1. / (TP + FN)\n",
    "        precision_test.append(precision)\n",
    "        recall_test.append(recall)\n",
    "        \n",
    "        # Built-in Logistic regression\n",
    "        regressor = linear_model.LogisticRegression()\n",
    "        regressor.fit(X_train, np.ravel(Y_train))\n",
    "        sklearn_acc_test.append(np.sum((regressor.predict(X_test) == np.ravel(Y_test))) * 1.0 / X_test.shape[0])\n",
    "        \n",
    "        print \"For fold\", i\n",
    "        print('Train Accuracy: {}'.format(acc_train[i]))\n",
    "        print('Test Accuracy: {}'.format(acc_test[i]))\n",
    "        print ('Sklearn Test Accuracy: {}').format(sklearn_acc_test[i])\n",
    "        print('Test Precision: {}').format(precision_test[i])\n",
    "        print('Test Recall: {}').format(recall_test[i])\n",
    "        print (\"\")\n",
    "        \n",
    "    print('Overall Mean Train Accuracy Across Folds: {}'.format(np.sum(acc_train)*1./len(acc_train)))\n",
    "    print('Overall Mean Test Accuracy Across Folds: {}'.format(np.sum(acc_test)*1. / len(acc_test)))\n",
    "    print('For sklearn, Overall Mean Test Accuracy Across Folds: {}'.format(np.sum(sklearn_acc_test)*1. / len(sklearn_acc_test)))\n",
    "\n",
    "    print('Overall Mean Test Precision Across Folds: {}').format(np.sum(precision_test)*1./len(precision_test))\n",
    "    print('Overall Mean Test Recall Across Folds: {}').format(np.sum(recall_test)*1./len(recall_test))\n",
    "    \n",
    "    print('std of train accuracy: {}'.format(np.std(np.array(acc_train), axis=0)))\n",
    "    print('std of test accuracy: {}'.format(np.std(np.array(acc_test), axis=0)))\n",
    "    \n",
    "    return acc_test, acc_train\n",
    "    \n",
    "def main():\n",
    "    print ('')\n",
    "    n_folds = 10\n",
    "    dataset = load_dataset(\"spambase.csv\")\n",
    "    dataset = dataset.astype(float)  \n",
    "    print('Spam dataset Logistic Regression')\n",
    "    logistic_regression(dataset, n_folds, lr=0.1, max_iter=5000, tolerance=1e-4)\n",
    "    print ('')\n",
    "    \n",
    "    dataset = load_dataset(\"breastcancer.csv\")\n",
    "    dataset = dataset.astype(float)  \n",
    "    print('Breast cancer dataset Logistic Regression')\n",
    "    logistic_regression(dataset, n_folds, lr=0.01, max_iter=5000, tolerance=.00001)\n",
    "    print ('')\n",
    "    \n",
    "    dataset = load_dataset(\"diabetes.csv\")\n",
    "    dataset = dataset.astype(float) \n",
    "    print('Diabetes dataset Logistic Regression')\n",
    "    logistic_regression(dataset, n_folds, lr=0.01, max_iter=5000, tolerance=.00001)\n",
    "    print ('')\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
