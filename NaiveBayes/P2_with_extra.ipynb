{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|V| = 100\n",
      "Calculating Bernoulli...\n",
      "Accuracy(%) = 18.0146568954\n",
      "Average precision across classes = 0.176948986341\n",
      "Average recall across classes = 0.178478309933\n",
      "Calculating Multinomial...\n",
      "Accuracy(%) = 23.890739507\n",
      "Average precision across classes = 0.235067020538\n",
      "Average recall across classes = 0.231143396779\n",
      "|V| = 500\n",
      "Calculating Bernoulli...\n",
      "Accuracy(%) = 36.4290473018\n",
      "Average precision across classes = 0.358637122429\n",
      "Average recall across classes = 0.422883329232\n",
      "Calculating Multinomial...\n",
      "Accuracy(%) = 48.5676215856\n",
      "Average precision across classes = 0.479834001384\n",
      "Average recall across classes = 0.500659381327\n",
      "|V| = 1000\n",
      "Calculating Bernoulli...\n",
      "Accuracy(%) = 46.2758161226\n",
      "Average precision across classes = 0.454990171489\n",
      "Average recall across classes = 0.5221675074\n",
      "Calculating Multinomial...\n",
      "Accuracy(%) = 60.0532978015\n",
      "Average precision across classes = 0.59241627597\n",
      "Average recall across classes = 0.608545108605\n",
      "|V| = 2500\n",
      "Calculating Bernoulli...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8d2581f6548a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-8d2581f6548a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mtest_label_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"20news-bydate/matlab/test.label\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m     \u001b[0macc_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-8d2581f6548a>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(train_file, label_file, test_file, test_label_file, V)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|V| = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculating Bernoulli...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_smoothing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBernoulli_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mpredict_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBernoulli_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_smoothing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy(%) = {}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_b\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-8d2581f6548a>\u001b[0m in \u001b[0;36mBernoulli_fit\u001b[0;34m(train_file, label_file, f, alpha, beta)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                     \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# MLE: 出现word i 且 属于j class的文本篇数 / 属于j class的文本篇数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from csv import reader\n",
    "from collections import Counter   \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'r') as dest_f:\n",
    "        data_iter = reader(dest_f, delimiter=' ', quotechar='\"')\n",
    "        data = [data for data in data_iter]\n",
    "        data_array = np.asarray(data)\n",
    "        data_array = data_array.astype(int)\n",
    "    return data_array\n",
    "\n",
    "def word_freq_list(train_data, frequency):\n",
    "    cnt = Counter()\n",
    "    for i in range(train_data.shape[0]):\n",
    "        cnt[train_data[i,1]] += train_data[i, 2]\n",
    "    if frequency == 'all':\n",
    "        word_freq_list = cnt.most_common()\n",
    "    else:\n",
    "        word_freq_list = cnt.most_common(frequency)\n",
    "    return word_freq_list\n",
    "\n",
    "def Bernoulli_fit(train_file, label_file, f, alpha, beta):\n",
    "    train_data = load_dataset(train_file)\n",
    "    train_label = load_dataset(label_file)\n",
    "    \n",
    "    # one-hot encoder\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(train_label)\n",
    "    \n",
    "    # calculate pi\n",
    "    class_frequency = np.sum(y, axis=0)\n",
    "    pi = class_frequency * 1. / len(train_label) \n",
    "   \n",
    "    # get the vocab_list\n",
    "    vocab_list = word_freq_list(train_data, f)\n",
    "\n",
    "    # use the vacab_list to make X(matrix): row -> docIdx, col -> {0, 1} whether the word appears in the doc\n",
    "    X = np.zeros((len(train_label), f))\n",
    "    vocab_dict = dict(vocab_list)\n",
    "    # list of the word_index\n",
    "    vocab_idx = [a[0] for a in vocab_list]\n",
    "    for row in range(train_data.shape[0]):\n",
    "        word_idx = train_data[row, 1]\n",
    "        if word_idx in vocab_dict:\n",
    "            docID = train_data[row, 0]\n",
    "            X[docID - 1, vocab_idx.index(word_idx)] = 1 \n",
    "\n",
    "    # calculate theta\n",
    "    vocab = f\n",
    "    classes = len(pi)\n",
    "    doc = len(train_label)\n",
    "    theta = np.zeros((vocab, classes))\n",
    "    theta_smoothing = np.zeros((vocab, classes))\n",
    "    for k in range(classes):\n",
    "        # MLE: 出现word i 且 属于j class的文本篇数 / 属于j class的文本篇数\n",
    "        for j in range(vocab):\n",
    "            cnt = 0\n",
    "            for i in range(doc):\n",
    "                if X[i, j] == 1 and y[i, k] == 1:\n",
    "                    cnt += 1\n",
    "            # MLE: 出现word i 且 属于j class的文本篇数 / 属于j class的文本篇数\n",
    "            # theta[j, k] = cnt * 1. / class_frequency[k]\n",
    "            theta_smoothing[j, k] = (cnt + 1) * 1./ (class_frequency[k] + 2)\n",
    "            \n",
    "            # MAP with smoothing\n",
    "            # theta_smoothing[j, k] = (cnt + alpha - 1 + 1) * 1. / (class_frequency[k] + beta + alpha - 2 + 2)\n",
    "        \n",
    "    return theta, theta_smoothing, pi, vocab_list\n",
    "\n",
    "def Bernoulli_predict(test_file, test_label_file, theta, theta_smoothing, pi, vocab_list):\n",
    "    test = load_dataset(test_file)\n",
    "    label = load_dataset(test_label_file)\n",
    "\n",
    "    predict = list()\n",
    "    max_prob = 0.0\n",
    "    predict_class = -1\n",
    "    classes = len(pi)\n",
    "\n",
    "    # make X_test(matrix)\n",
    "    X_test = np.zeros((len(label), len(vocab_list)))\n",
    "    vocab_dict = dict(vocab_list)\n",
    "\n",
    "    # list of the word_index\n",
    "    vocab_idx = [a[0] for a in vocab_list]\n",
    "    for row in range(test.shape[0]):\n",
    "        word_idx = test[row, 1]\n",
    "        if word_idx in vocab_dict:\n",
    "            docID = test[row, 0]\n",
    "            X_test[docID - 1, vocab_idx.index(word_idx)] = 1\n",
    "    \n",
    "    for i in range(X_test.shape[0]):\n",
    "        # iterate every doc, calculate joint probablity of words, classes\n",
    "        max_prob = 0.0\n",
    "        predict_class = -1\n",
    "        for c in range(classes):\n",
    "            conditional_prob = 1.0\n",
    "            for j in range(X_test.shape[1]):\n",
    "                if X_test[i, j] == 0:\n",
    "                    conditional_prob += np.log(1 - theta_smoothing[j, c])\n",
    "                else:\n",
    "                    conditional_prob += np.log(theta_smoothing[j, c])\n",
    "\n",
    "            if c == 0 or conditional_prob + np.log(pi[c]) > max_prob:\n",
    "                max_prob = conditional_prob + np.log(pi[c])\n",
    "                predict_class = c + 1 #\n",
    "        predict.append(predict_class)\n",
    "    label = label[:, -1]\n",
    "    acc = sum(predict == label) * 1. / len(predict)  \n",
    "    \n",
    "#         # make confusion matrix\n",
    "    confusion_matrix = np.zeros((theta.shape[1], theta.shape[1]))\n",
    "    for i in range(len(predict)):\n",
    "        confusion_matrix[int(label[i])-1, int(predict[i])-1] += 1\n",
    "    \n",
    "    # row: label; col: predict\n",
    "    total = np.sum(confusion_matrix)\n",
    "    precision = []\n",
    "    recall = []\n",
    "    TP = 0\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        TP += confusion_matrix[i, i]\n",
    "        curr_label = 0\n",
    "        curr_pred = 0\n",
    "        for j in range(confusion_matrix.shape[0]):\n",
    "            curr_label += confusion_matrix[i, j]\n",
    "            curr_pred += confusion_matrix[j, i]\n",
    "        if curr_label != 0:\n",
    "            precision.append(confusion_matrix[i, i] * 1. / curr_label)\n",
    "        if curr_pred != 0:\n",
    "            recall.append(confusion_matrix[i, i] * 1. / curr_pred)\n",
    "    precision = np.array(precision)\n",
    "    recall = np.array(recall)\n",
    "    acc = TP * 1. / total\n",
    "    return predict, acc, precision, recall\n",
    "\n",
    "def Multinomial_fit(train_file, label_file, f, alpha=1, alpha_i=2):\n",
    "    train_data = load_dataset(train_file)\n",
    "    train_label = load_dataset(label_file)\n",
    "\n",
    "    # one-hot encoder\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(train_label)\n",
    "    # calculate pi\n",
    "    class_frequency = np.sum(y, axis=0)\n",
    "    pi = class_frequency * 1. / len(train_label) \n",
    "\n",
    "    # get the vocab_list\n",
    "    vocab_list = word_freq_list(train_data, f)\n",
    "\n",
    "    # use the vacab_list to make X(matrix): row -> docIdx, col -> {0, 1} whether the word appears in the doc\n",
    "    X = np.zeros((len(train_label), f))\n",
    "    vocab_dict = dict(vocab_list)\n",
    "    # list of the word_index\n",
    "    vocab_idx = [a[0] for a in vocab_list]\n",
    "    for row in range(train_data.shape[0]):\n",
    "        word_idx = train_data[row, 1]\n",
    "        if word_idx in vocab_dict:\n",
    "            docID = train_data[row, 0]\n",
    "            X[docID - 1, vocab_idx.index(word_idx)] = train_data[row, 2] \n",
    "    \n",
    "    # calculate theta\n",
    "    vocab = f\n",
    "    classes = len(pi)\n",
    "    doc = len(train_label)\n",
    "    theta = np.zeros((vocab, classes))\n",
    "    theta_smoothing = np.zeros((vocab, classes))\n",
    "    \n",
    "    for k in range(classes):\n",
    "        # word i 在k class的频率 / k class的总词频\n",
    "        for j in range(vocab):\n",
    "            cnt = 0\n",
    "            total_frequency = 0\n",
    "            for i in range(doc):\n",
    "                if y[i, k] == 1:\n",
    "                    total_frequency += np.sum(X[i, :])\n",
    "                    cnt += X[i, j]\n",
    "            # MLE\n",
    "            # theta[j, k] = cnt * 1. / total_frequency\n",
    "            theta_smoothing[j, k] = (cnt + alpha) * 1. / (total_frequency + vocab) \n",
    "            \n",
    "            # MAP\n",
    "            #theta_smoothing[j, k] = (cnt + alpha_i - 1 + 1) * 1. / (total_frequency + vocab * alpha_i - vocab + vocab)\n",
    "            #theta_smoothing[j, k] = (cnt + alpha_i) * 1. / (total_frequency + vocab * alpha_i)\n",
    "    return theta, theta_smoothing, pi, vocab_list\n",
    "\n",
    "def Multinomial_predict(test_file, test_label_file, theta, theta_smoothing, pi, vocab_list):\n",
    "    test = load_dataset(test_file)\n",
    "    label = load_dataset(test_label_file)\n",
    "\n",
    "    predict = list()\n",
    "    max_prob = 0.0\n",
    "    predict_class = -1\n",
    "    classes = len(pi)\n",
    "\n",
    "    # make X_test(matrix)\n",
    "    X_test = np.zeros((len(label), len(vocab_list)))\n",
    "    vocab_dict = dict(vocab_list)\n",
    "\n",
    "    # list of the word_index\n",
    "    vocab_idx = [a[0] for a in vocab_list]\n",
    "    for row in range(test.shape[0]):\n",
    "        word_idx = test[row, 1]\n",
    "        if word_idx in vocab_dict:\n",
    "            docID = test[row, 0]\n",
    "            X_test[docID - 1, vocab_idx.index(word_idx)] = test[row, 2] \n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "    # iterate every doc, calculate joint probablity of words, classes\n",
    "        max_prob = 0.0\n",
    "        predict_class = -1\n",
    "        for c in range(classes):\n",
    "            conditional_prob = 0.0 \n",
    "            for j in range(X_test.shape[1]): \n",
    "                ## using log likelihood to prevent underflow and increase efficiency \n",
    "                conditional_prob += np.log(theta_smoothing[j, c]) * X_test[i, j]\n",
    "\n",
    "            if c == 0 or (conditional_prob + np.log(pi[c])) > max_prob:\n",
    "                max_prob = conditional_prob + np.log(pi[c])\n",
    "\n",
    "                predict_class = c + 1\n",
    "        predict.append(predict_class)\n",
    "    label = label[:, -1] \n",
    "    acc = sum(predict == label) * 1. / len(predict) \n",
    "   \n",
    "    # make confusion matrix\n",
    "    confusion_matrix = np.zeros((theta.shape[1], theta.shape[1]))\n",
    "    for i in range(len(predict)):\n",
    "        confusion_matrix[int(label[i])-1, int(predict[i])-1] += 1\n",
    "    \n",
    "    # row: label; col: predict\n",
    "    total = np.sum(confusion_matrix)\n",
    "    precision = []\n",
    "    recall = []\n",
    "    TP = 0\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        TP += confusion_matrix[i, i]\n",
    "        curr_label = 0\n",
    "        curr_pred = 0\n",
    "        for j in range(confusion_matrix.shape[0]):\n",
    "            curr_label += confusion_matrix[i, j]\n",
    "            curr_pred += confusion_matrix[j, i]\n",
    "        if curr_label != 0:\n",
    "            precision.append(confusion_matrix[i, i] * 1. / curr_label)\n",
    "        if curr_pred != 0:\n",
    "            recall.append(confusion_matrix[i, i] * 1. / curr_pred)\n",
    "    precision = np.array(precision)\n",
    "    recall = np.array(recall)\n",
    "    acc = TP * 1. / total\n",
    "    return predict, acc, precision, recall\n",
    "\n",
    "def evaluation(train_file, label_file, test_file, test_label_file, V):\n",
    "    Bernoulli_acc = list()\n",
    "    Multinomial_acc = list()\n",
    "    Bernoulli_precision = list()\n",
    "    Multinomial_precision = list()\n",
    "    Bernoulli_recall = list()\n",
    "    Multinomial_recall = list()\n",
    "    \n",
    "    for freq in V:\n",
    "        print(\"|V| = {}\".format(freq))\n",
    "        print(\"Calculating Bernoulli...\")\n",
    "        theta, theta_smoothing, pi, vocab_list = Bernoulli_fit(train_file, label_file, freq, alpha=2, beta=2)\n",
    "        predict_b, acc_b, precision_b, recall_b = Bernoulli_predict(test_file, test_label_file, theta, theta_smoothing, pi, vocab_list)\n",
    "        print(\"Accuracy(%) = {}\").format(acc_b * 100)\n",
    "        print(\"Average precision across classes = {}\").format(np.mean(precision_b))\n",
    "        print(\"Average recall across classes = {}\").format(np.mean(recall_b))\n",
    "        \n",
    "        Bernoulli_acc.append(acc_b*100)\n",
    "        Bernoulli_precision.append(precision_b)\n",
    "        Bernoulli_recall.append(recall_b) \n",
    "        \n",
    "        print(\"Calculating Multinomial...\")\n",
    "        theta, theta_smoothing, pi, vocab_list = Multinomial_fit(train_file, label_file, freq, alpha=1, alpha_i=2)\n",
    "        predict_m, acc_m, precision_m, recall_m = Multinomial_predict(test_file, test_label_file, theta, theta_smoothing, pi, vocab_list)\n",
    "        print(\"Accuracy(%) = {}\").format(acc_m * 100)\n",
    "        print(\"Average precision across classes = {}\").format(np.mean(precision_m))\n",
    "        print(\"Average recall across classes = {}\").format(np.mean(recall_m))\n",
    "        \n",
    "        Multinomial_acc.append(acc_m*100)\n",
    "        Multinomial_precision.append(precision_m)\n",
    "        Multinomial_recall.append(recall_m)\n",
    "        \n",
    "    return Bernoulli_acc, Multinomial_acc, Bernoulli_precision, Multinomial_precision, Bernoulli_recall, Multinomial_recall \n",
    "\n",
    "def plot(acc_b_list, acc_m_list, precision_b_list, precision_m_list, recall_b_list, recall_m_list, V, idx):\n",
    "    plt.figure(1)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Accuracy((%))')\n",
    "    plt.plot(V, acc_m_list, linestyle='-', marker='o', label=\"multivariate event\")\n",
    "    plt.plot(V, acc_b_list, linestyle='-', marker='o', label=\"multivariate Bernoulli\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Accuracy Against Frequency')\n",
    "    plt.grid()\n",
    "                                                                                                                                                                 \n",
    "    \n",
    "    # plot precision\n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(len(precision_m_list[idx]))\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, precision_b_list[idx], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='g',\n",
    "                     label='multivariate Bernoulli')\n",
    "    \n",
    "    rects2 = plt.bar(index + bar_width, precision_m_list[idx], bar_width,\n",
    "             alpha=opacity,\n",
    "             color='b',\n",
    "             label='multivariate event')                                                         \n",
    "\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision over 20 classes')\n",
    "    plt.xticks(index + bar_width, list(range(1,21)))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # plot recall\n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(len(recall_m_list[idx]))\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = plt.bar(index, recall_b_list[idx], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='g',\n",
    "                     label='multivariate Bernoulli')\n",
    "    \n",
    "    rects2 = plt.bar(index + bar_width, recall_m_list[idx], bar_width,\n",
    "             alpha=opacity,\n",
    "             color='b',\n",
    "             label='multivariate event')                                                         \n",
    "\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Recall over 20 Classes')\n",
    "    plt.xticks(index + bar_width, list(range(1,21)))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "                                                              \n",
    "def main():\n",
    "    train_file = \"20news-bydate/matlab/train.data\"\n",
    "    label_file = \"20news-bydate/matlab/train.label\"\n",
    "    test_file = \"20news-bydate/matlab/test.data\"\n",
    "    test_label_file = \"20news-bydate/matlab/test.label\"\n",
    "    V = [100, 500, 1000, 2500, 5000, 7500, 10000, 12500, 25000, 50000, \"all\"]\n",
    "    acc_b, acc_m, precision_b, precision_m, recall_b, recall_m = evaluation(train_file, label_file, test_file, test_label_file, V)\n",
    "    \n",
    "    plot(acc_b, acc_m, precision_b, precision_m, recall_b, recall_m, V, idx=0)\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
